# WSL2 + GPU Setup Summary - November 16, 2025

## What We Accomplished

### 1. Environment Setup ‚úÖ
- **Platform**: WSL2 on Windows (Linux kernel 6.6.87.2-microsoft)
- **GPU**: NVIDIA GeForce RTX 3050, 8GB VRAM, CUDA 13.0
- **Key Finding**: WSL2 provides full Linux environment, enabling cuML access

### 2. Installation ‚úÖ
- Created Python 3.13 virtual environment with `uv`
- Installed all dependencies including GPU support: `uv pip install -e ".[gpu]"`
- Installed packages:
  - cuML 25.10.0 (GPU-accelerated ML)
  - CuPy 13.6.0 (GPU arrays)
  - CUDA toolkit libraries (via pip packages)

### 3. Library Path Configuration ‚úÖ
- **Problem**: CUDA libraries not in system path
- **Solution**: Created `activate_gpu.sh` script that:
  - Activates virtual environment
  - Adds CUDA libraries from venv to `LD_LIBRARY_PATH`
  - Usage: `source activate_gpu.sh` before running GPU code

### 4. Critical Bug Fix: Float32 Precision Issue ‚úÖ
- **Problem Discovered**: cuML converts float64 input to float32 internally
- **Impact**: With large UTM coordinates (e.g., Y=6,655,000), precision loss caused:
  - Overflow in squared distance calculations
  - `inf` and `nan` values in results
  - Incorrect distance measurements

- **Solution Implemented**: Center point clouds before GPU computation
  ```python
  # In compute_c2c():
  target_center = np.mean(target, axis=0)
  target_centered = target - target_center
  source_centered = source - target_center
  # Now coordinates are small (~0-1000m range) ‚Üí no overflow
  ```

- **Result**: 
  - ‚úÖ No more overflow errors
  - ‚úÖ GPU and CPU distances match exactly
  - ‚úÖ Numerically stable across all dataset sizes

### 5. Performance Analysis

**Current Performance (with centering fix):**
| Points  | GPU Time | CPU Time | Speedup | Status |
|---------|----------|----------|---------|--------|
| 10K     | 0.039s   | 0.009s   | 0.23x   | Slower |
| 50K     | 0.221s   | 0.052s   | 0.24x   | Slower |
| 100K    | 0.277s   | 0.113s   | 0.41x   | Slower |
| 200K    | 0.474s   | 0.246s   | 0.52x   | Slower |

**Why is GPU slower?**
1. **Initialization overhead**: cuML model creation and GPU memory allocation
2. **Data transfers**: CPU ‚Üí GPU ‚Üí CPU for each call
3. **Small dataset overhead**: GPU shines at 1M+ points, not 10K-200K

**Direct cuML benchmark (100K points):**
- Pure cuML (data on GPU): 0.15s
- With transfers: 0.10s  
- sklearn CPU: 0.15s

‚Üí cuML itself is fast, but our usage pattern adds overhead

## Next Steps

### Option 1: Accept Current State (Recommended for Now)
- GPU infrastructure is working correctly
- Numerical issues fixed
- Document that GPU benefits require larger datasets (1M+ points)
- Focus on other optimizations

### Option 2: Optimize GPU Usage
- Keep GPU model alive between calls (model pooling)
- Batch multiple C2C computations together
- Use GPU for tiled processing where we process many tiles

### Option 3: Alternative GPU Library
- Try FAISS (Facebook's similarity search library)
- Works on both Windows and Linux
- Might have better performance characteristics for our use case

## Files Modified

1. `activate_gpu.sh` - Created activation script with CUDA library paths
2. `src/terrain_change_detection/detection/change_detection.py` - Added point cloud centering for GPU path
3. `debug_gpu_slowdown.py` - Created diagnostic script

## How to Use

```bash
# Activate environment with GPU support
source activate_gpu.sh

# Run tests
pytest tests/test_gpu_neighbors.py -v

# Run performance benchmarks
python scripts/test_gpu_c2c_performance.py --max-points 100000
```

## Key Learnings

1. **WSL2 = Full GPU Access**: Unlike native Windows, WSL2 provides complete Linux environment
2. **Float32 Precision Matters**: Large coordinates require centering to avoid overflow
3. **GPU Overhead**: For small-medium datasets, initialization overhead dominates
4. **Library Path Critical**: Must set LD_LIBRARY_PATH for CUDA libraries from pip packages

## Status

- ‚úÖ GPU infrastructure complete and working
- ‚úÖ cuML successfully installed and tested
- ‚úÖ Numerical stability fixed
- ‚ö†Ô∏è  Performance needs optimization for small-medium datasets
- üìä Ready for large-scale testing (1M+ points)

Last updated: November 16, 2025
