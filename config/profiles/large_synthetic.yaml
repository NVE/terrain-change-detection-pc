# Configuration profile for large synthetic datasets (out-of-core testing)
#
# Use this to test parallelization performance with realistic dataset sizes (50M+ points).
# This profile enables out-of-core processing and parallel execution to validate
# the scalability infrastructure.
#
# Generate the dataset first:
#   uv run scripts/generate_large_synthetic_laz.py
#
# Then run the workflow:
#   uv run scripts/run_workflow.py --config config/profiles/large_synthetic.yaml

paths:
  # Root folder containing large synthetic dataset
  base_dir: data/large_synthetic

preprocessing:
  # Keep only LAS class 2 (ground)
  ground_only: true
  # Optional list of LAS classification codes to include
  classification_filter: [2]

discovery:
  # Subdirectory names used when discovering datasets
  data_dir_name: data
  metadata_dir_name: metadata

alignment:
  # ICP/registration parameters
  max_iterations: 100
  tolerance: 1.0e-6
  max_correspondence_distance: 2.0
  # Larger subsample for better alignment with big datasets
  subsample_size: 100000
  coarse:
    # Disabled - no misalignment in synthetic data
    enabled: false
    # Coarse alignment method
    method: phase
    # Voxel size (m) for coarse features/downsampling
    voxel_size: 3.0
    # Cell size (m) for phase correlation grid
    phase_grid_cell: 3.0

detection:
  dod:
    enabled: true
    # DEM grid resolution in meters
    cell_size: 2.0
    # Aggregation for per-cell elevation
    aggregator: mean
  c2c:
    enabled: false
    # Algorithm: euclidean | vertical_plane
    mode: euclidean
    # Upper bound for sampling/limits
    max_points: 10000
    # Search radius (m) - required for streaming C2C
    max_distance: 10.0
    # Local modeling parameters (for mode=vertical_plane)
    radius: null
    k_neighbors: 20
    min_neighbors: 6
  m3c2:
    enabled: true
    # Use automatic parameter tuning (true) or fixed parameters below (false)
    use_autotune: true
    # Percentage of reference ground points for M3C2 core points
    core_points_percent: 10.0
    autotune:
      source: header
      target_neighbors: 20
      max_depth_factor: 0.6
      min_radius: 2.0
      max_radius: 50.0
    # Fixed parameters (used only when use_autotune: false). Leave as null to accept defaults.
    fixed:
      radius: null          # If set, projection_scale=cylinder_radius=radius
      normal_scale: null    # Defaults to radius when null
      depth_factor: null    # If null, falls back to autotune.max_depth_factor
    ep:
      # Number of worker processes/threads; null = OS default
      workers: null

visualization:
  # plotly | pyvista | pyvistaqt
  backend: plotly
  # Max points to draw for interactive views
  sample_size: 100000

logging:
  level: INFO
  # Path to log file; null = stdout only
  file: logs/large_synthetic_processing.log

performance:
  # Set to an integer to pin NumPy threads; 'auto' lets NumPy decide
  numpy_threads: auto

outofcore:
  # Master switch for streaming/tiling out-of-core workflows
  # ENABLED for large dataset processing
  enabled: true
  # Inner tile size in meters (processing region per tile)
  tile_size_m: 500.0
  # Halo width in meters (buffer around each tile to avoid edge effects)
  halo_m: 30.0
  # Max points per streaming chunk (memory/performance trade-off)
  chunk_points: 2000000
  # Use streaming mode in preprocessing when out-of-core is enabled
  streaming_mode: true
  # Save transformed LAZ files during alignment to disk
  save_transformed_files: false
  # Output directory for transformed files; null = auto-generate
  output_dir: null
  # Directory for memory-mapped arrays in mosaicking; null = in-memory
  memmap_dir: null

parallel:
  # Master switch for CPU parallelization
  # ENABLED to test parallel performance with large dataset
  enabled: true
  # Number of worker processes; null = auto-detect (cpu_count - 1)
  n_workers: null
  # Soft memory limit (GB) to guide worker count; null = no limit
  memory_limit_gb: null

gpu:
  # Master switch for GPU acceleration (graceful CPU fallback if unavailable)
  # WARNING: Cannot be used simultaneously with parallel.enabled due to CUDA fork limitations
  enabled: false
  # Max GPU memory to use in GB; null = auto-detect (80% of available)
  gpu_memory_limit_gb: null
  # Automatically fall back to CPU if GPU initialization fails
  fallback_to_cpu: true
  # Use GPU for C2C nearest neighbor searches (10-50x speedup potential)
  use_for_c2c: true
  # Use GPU for data preprocessing (transformations, filtering)
  use_for_preprocessing: true
  # Use GPU for ICP spatial alignment (experimental; not recommended - see GPU_SETUP_GUIDE.md)
  use_for_alignment: false
  # GPU batch size for operations; null = auto-calculate based on memory
  batch_size: null
